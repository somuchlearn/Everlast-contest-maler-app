# ğŸ¨ PaintVoice AI

**Intelligente Angebotserstellung fÃ¼r Maler â€“ per Sprache und Text**

> ğŸ† Eingereicht fÃ¼r den Developer Contest: Voice-to-Text Desktop App

---

## ğŸ¯ Das Problem

**Maler und Handwerker verlieren tÃ¤glich 1-2 Stunden mit Angebotserstellung.**

Der typische Ablauf heute:
1. Kunde anrufen / Vor-Ort-Termin
2. Notizen auf Papier oder Handy
3. ZurÃ¼ck ins BÃ¼ro
4. Fotos durchschauen
5. FlÃ¤chen berechnen
6. Preise nachschlagen
7. Angebot in Word/Excel tippen
8. PDF erstellen und versenden

**Das Ergebnis:** FehleranfÃ¤llig, zeitaufwÃ¤ndig, frustrierend.

---

## ğŸ’¡ Die LÃ¶sung

**PaintVoice AI** transformiert den gesamten Prozess in einen einzigen, nahtlosen Workflow:

```
ğŸ¤ Sprache  +  ğŸ“¸ Fotos  +  ğŸ“ Text  â†’  ğŸ¤– Lokale KI  â†’  ğŸ“„ Fertiges Angebot
```

### So funktioniert's:

1. **Hotkey drÃ¼cken** (`âŒ˜â‡§R`) â€“ App ist sofort bereit
2. **Sprechen**: *"Wohnzimmer 25 Quadratmeter, WÃ¤nde und Decke streichen, drei Fenster, Risse in der SÃ¼dwand"*
3. **Optional**: Fotos vom Raum hinzufÃ¼gen
4. **Ein Klick** â†’ VollstÃ¤ndiges Angebot mit:
    - Detaillierten Positionen
    - Korrekten Preisen (nach Handwerksstandard)
    - Professioneller KI-Analyse
    - PDF-Export

**Zeitersparnis: 45-60 Minuten pro Angebot.**

---

## âœ¨ Features

| Feature | Beschreibung |
|---------|-------------|
| ğŸ¤ **Voice-to-Text** | Lokale Whisper-Transkription im Browser â€“ keine Cloud nÃ¶tig |
| ğŸ“¸ **Foto-Upload** | Fotos werden zum Angebot zugeordnet |
| âŒ¨ï¸ **Global Hotkey** | `âŒ˜â‡§R` / `Ctrl+Shift+R` â€“ funktioniert aus jeder Anwendung |
| ğŸ¤– **Lokale KI** | Ollama mit Mistral â€“ Angebot wird lokal berechnet |
| â˜ï¸ **Cloud-Fallback** | Falls Ollama nicht verfÃ¼gbar â†’ automatischer Fallback auf OpenAI GPT-4o |
| ğŸ“„ **PDF-Export** | Professionelle Angebote, sofort versandfertig |
| ğŸŒ™ **Dark Mode UI** | Modernes, augenschonendes Design |
| ğŸ”’ **Offline-tÃ¼chtig** | Transkription lÃ¤uft komplett lokal, Enrichment per Ollama ohne Internet |

---

## ğŸ—ï¸ Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PaintVoice AI                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Tauri     â”‚    â”‚  Next.js    â”‚    â”‚  TypeScript â”‚     â”‚
â”‚  â”‚   (Rust)    â”‚â—„â”€â”€â–ºâ”‚   Frontend  â”‚â—„â”€â”€â–ºâ”‚   Logic     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                  â”‚                   â”‚            â”‚
â”‚         â–¼                  â–¼                   â–¼            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Global    â”‚    â”‚   Whisper   â”‚    â”‚   Ollama    â”‚     â”‚
â”‚  â”‚   Hotkey    â”‚    â”‚   (lokal)   â”‚    â”‚   (lokal)   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                               â”‚             â”‚
â”‚                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚                                    â”‚  Fallback wenn  â”‚      â”‚
â”‚                                    â”‚  Ollama offline â”‚      â”‚
â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                             â–¼               â”‚
â”‚                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚                                    â”‚  OpenAI     â”‚          â”‚
â”‚                                    â”‚  GPT-4o     â”‚          â”‚
â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technologie-Stack

| Komponente | Technologie | Warum? |
|------------|-------------|--------|
| **Desktop Runtime** | Tauri 2.0 + Rust | Schnell, sicher, klein (~10MB vs ~150MB Electron) |
| **Framework** | Next.js 15 + React 19 | Statischer Export fÃ¼r Tauri, moderna Komponentenstruktur |
| **Voice-to-Text** | Whisper via @huggingface/transformers | LÃ¤uft lokal im Browser (WASM/WebGPU), keine API nÃ¶tig |
| **LLM Enrichment** | Ollama (Mistral) | Lokales Modell, OpenAI-kompatibler API, keine Cloud |
| **Cloud-Fallback** | OpenAI GPT-4o | Backup wenn Ollama nicht verfÃ¼gbar |
| **PDF-Generation** | jsPDF | Client-seitig, keine Server-AbhÃ¤ngigkeit |

---

## ğŸ”„ Datenfluss

```
User Input                    Processing                     Output
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Sprache â”‚â”€â”€â–º â”‚  Whisper (lokal)    â”‚â”€â”€â–º Transkript â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  @huggingface/      â”‚                 â”‚
                â”‚  transformers       â”‚                 â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
                                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Fotos   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º    â”‚  Ollama  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚ (lokal)  â”‚
                                                â”‚  oder    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚ OpenAI   â”‚
â”‚   Text   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º    â”‚ (Backup) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                                                     â”‚
                                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PDF    â”‚ â—„â”€â”€â”€ â”‚  UI      â”‚ â—„â”€â”€â”€ â”‚  Parse   â”‚â—„â”€â”‚  JSON    â”‚
â”‚  Export  â”‚      â”‚  Render  â”‚      â”‚  + Calc  â”‚  â”‚ Response â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Installation & Setup

### Voraussetzungen

- Node.js 18+
- Rust (via rustup)
- [Ollama](https://ollama.ai) installiert
- macOS / Windows / Linux

### 1. Repository klonen

```bash
git clone https://github.com/somuchlearn/Everlast-contest-maler-app.git
cd Everlast-contest-maler-app
```

### 2. Ollama-Modell pullen

```bash
ollama pull mistral
```

> Das Modell (~4GB) wird einmal heruntergeladen und lokal gespeichert. Ollama muss beim Betrieb im Hintergrund laufen.

### 3. Dependencies installieren

```bash
npm install
```

### 4. Environment konfigurieren (optional)

Erstelle `.env` im Root-Verzeichnis fÃ¼r den Cloud-Fallback:

```env
NEXT_PUBLIC_OPENAI_API_KEY=sk-your-openai-api-key
```

> Ohne diesen Key lÃ¤uft die App trotzdem â€“ solange Ollama lokal lÃ¤uft, wird OpenAI nicht benÃ¶tigt.

### 5. Development starten

```bash
npm run tauri dev
```

### 6. Production Build

```bash
npm run tauri build
```

Das Build-Artefakt liegt in `src-tauri/target/release/bundle/`.

> **Beim ersten Start** wird das Whisper-Modell (~244MB) automatisch heruntergeladen und gecacht. Danach lÃ¤uft die Transkription komplett offline.

---

## âŒ¨ï¸ Bedienung

| Aktion | Shortcut / UI |
|--------|---------------|
| App aktivieren | `âŒ˜â‡§R` (Mac) / `Ctrl+Shift+R` (Win/Linux) |
| Aufnahme starten | Button "ğŸ™ï¸ Aufnahme starten" oder Hotkey |
| Aufnahme stoppen | Button "Aufnahme stoppen" oder Hotkey erneut |
| Fotos hinzufÃ¼gen | Drag & Drop oder Klick auf Upload-Zone |
| Angebot erstellen | Button "Angebot erstellen â†’" |
| PDF exportieren | Button "ğŸ“„ PDF" im Ergebnis |
| Neues Angebot | Button "â†º Neu" |

---

## ğŸ¨ Design-Entscheidungen

### 1. Warum Tauri statt Electron?

| Kriterium | Tauri | Electron |
|-----------|-------|----------|
| Bundle-GrÃ¶ÃŸe | ~10 MB | ~150 MB |
| RAM-Verbrauch | ~50 MB | ~300 MB |
| Startup-Zeit | <1s | 2-3s |
| Sicherheit | Rust-basiert | Chromium-basiert |

**Fazit:** FÃ¼r eine App die "nahtlos im Workflow" sein soll, ist Performance entscheidend.

### 2. Warum lokale KI statt Cloud-Only?

Das Briefing fordert eine **"eigenstÃ¤ndige Desktop-Applikation"**. Das bedeutet minimale AbhÃ¤ngigkeiten von externen Diensten:

- **Transkription lokal:** Whisper lÃ¤uft Ã¼ber `@huggingface/transformers` direkt im Browser via WASM/WebGPU. Das Modell wird beim ersten Start gecacht â€“ danach kein Internet mehr nÃ¶tig.
- **Enrichment lokal:** Ollama fÃ¼hrt ein lokales Mistral-Modell aus. Die gesamte Angebot-Kalkulation passiert auf dem Rechner des Nutzers.
- **Fallback-Architektur:** Falls Ollama nicht verfÃ¼gbar ist, wird automatisch auf OpenAI GPT-4o zurÃ¼ckgefallen. Die App funktioniert niemals nicht.

### 3. Warum Ollama + Mistral?

- **OpenAI-kompatibler API:** Der gleiche Code arbeitet mit Ollama lokal und OpenAI als Fallback â€“ minimale Duplikation.
- **Mistral:** Starkes Modell fÃ¼r Deutsch, verlÃ¤sslich bei strukturierter JSON-Ausgabe.
- **Einfaches Setup:** Ein Befehl (`ollama pull mistral`), dann lÃ¤uft es.

### 4. UI/UX Entscheidungen

- **Dark Mode:** Reduziert Augenbelastung bei lÃ¤ngerer Nutzung
- **Minimalistisches Design:** Fokus auf Funktion, keine Ablenkung
- **Inline-Feedback:** Status, Transkript, Fehler direkt sichtbar
- **Model-Status-Indikator:** Zeigt ob das Whisper-Modell geladen ist

---

## ğŸ“Š Beispiel-Output

**Eingabe (Sprache):**
> "Schlafzimmer 18 Quadratmeter, WÃ¤nde streichen weiÃŸ, Decke auch, ein Fenster, TÃ¼r, kleiner Riss an der Nordwand"

**Ausgabe:**

| Position | Menge | Einheit | Preis | Summe |
|----------|-------|---------|-------|-------|
| Abkleben Fenster, TÃ¼r | 2 | Stk | 4,50 â‚¬ | 9,00 â‚¬ |
| Risse ausbessern | 1 | Stk | 30,00 â‚¬ | 30,00 â‚¬ |
| Grundierung WÃ¤nde | 48 | mÂ² | 8,00 â‚¬ | 384,00 â‚¬ |
| Wandanstrich weiÃŸ 2-fach | 48 | mÂ² | 14,00 â‚¬ | 672,00 â‚¬ |
| Deckenanstrich weiÃŸ | 18 | mÂ² | 13,00 â‚¬ | 234,00 â‚¬ |
| **Netto** | | | | **1.329,00 â‚¬** |
| **MwSt. 19%** | | | | **252,51 â‚¬** |
| **Brutto** | | | | **1.581,51 â‚¬** |

---

## ğŸ™ Danksagungen

- [Tauri](https://tauri.app/) â€“ Desktop-Framework
- [Next.js](https://nextjs.org/) â€“ Web-Framework
- [@huggingface/transformers](https://github.com/huggingface/transformers.js) â€“ Lokale Whisper-Inferenz im Browser
- [Ollama](https://ollama.ai/) â€“ Lokale LLM-AusfÃ¼hrung
- [jsPDF](https://github.com/parallax/jsPDF) â€“ Client-seitige PDF-Generierung

---

<div align="center">

**ğŸ¨ PaintVoice AI** â€“ *Sprich dein Angebot.*

</div>
