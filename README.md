# ğŸ¨ MalerVoice AI

**Intelligente Angebotserstellung fÃ¼r Maler â€“ per Sprache und Text**

> ğŸ† Eingereicht fÃ¼r den Developer Contest: Voice-to-Text Desktop App

---

## ğŸ¯ Das Problem

**Maler und Handwerker verlieren tÃ¤glich 1-2 Stunden mit Angebotserstellung.**

Der typische Ablauf heute:
1. Kunde anrufen / Vor-Ort-Termin
2. Notizen auf Papier oder Handy
3. ZurÃ¼ck ins BÃ¼ro
4. Fotos durchschauen
5. FlÃ¤chen berechnen
6. Preise nachschlagen
7. Angebot in Word/Excel tippen
8. PDF erstellen und versenden

**Das Ergebnis:** FehleranfÃ¤llig, zeitaufwÃ¤ndig, frustrierend.

---

## ğŸ’¡ Die LÃ¶sung

**MalerVoice AI** transformiert den gesamten Prozess in einen einzigen, nahtlosen Workflow:

```
ğŸ¤ Sprache  +  ğŸ“¸ Fotos  +  ğŸ“ Text  â†’  ğŸ¤– KI (lokal oder Cloud)  â†’  ğŸ“„ Fertiges Angebot
```

### So funktioniert's:

1. **Hotkey drÃ¼cken** (`âŒ˜â‡§R`) â€“ App ist sofort bereit
2. **Sprechen**: *"Wohnzimmer 25 Quadratmeter, WÃ¤nde und Decke streichen, drei Fenster, Risse in der SÃ¼dwand"*
3. **Optional**: Fotos vom Raum hinzufÃ¼gen
4. **Ein Klick** â†’ VollstÃ¤ndiges Angebot mit:
    - Detaillierten Positionen
    - Korrekten Preisen (nach Handwerksstandard)
    - Professioneller KI-Analyse
    - PDF-Export

**Zeitersparnis: 45-60 Minuten pro Angebot.**

---

## âœ¨ Features

| Feature | Beschreibung |
|---------|-------------|
| ğŸ¤ **Voice-to-Text** | Online: OpenAI Whisper API Â· Offline: lokaler Whisper im Browser (WASM) |
| ğŸ“¸ **Foto-Analyse** | Online: Bilder werden per n8n an Google Gemini gesendet fÃ¼r visuelle Kalkulation |
| âŒ¨ï¸ **Global Hotkey** | `âŒ˜â‡§R` / `Ctrl+Shift+R` â€“ funktioniert aus jeder Anwendung |
| ğŸ¤– **KI-Enrichment** | Online: n8n + Gemini Â· Offline: Ollama (Mistral) mit OpenAI-Fallback |
| ğŸ”„ **Online/Offline** | Ein Toggle wechselt zwischen Cloud- und lokal-Modus |
| ğŸ“„ **PDF-Export** | Professionelle Angebote, sofort versandfertig |
| ğŸŒ™ **Dark Mode UI** | Modernes, augenschonendes Design |

---

## ğŸ—ï¸ Architektur

![Architektur](docs/architektur.svg)

### Technologie-Stack

| Komponente | Technologie | Warum? |
|------------|-------------|--------|
| **Desktop Runtime** | Tauri 2.0 + Rust | Schnell, sicher, klein (~10MB vs ~150MB Electron) |
| **Framework** | Next.js 15 + React 19 | Statischer Export fÃ¼r Tauri, moderna Komponentenstruktur |
| **Voice (Offline)** | Whisper via @huggingface/transformers | LÃ¤uft lokal im Browser (WASM), keine API nÃ¶tig |
| **Voice (Online)** | OpenAI Whisper API | Schnellere Transkription, kein lokaler Modell-Download |
| **Enrichment (Offline)** | Ollama (Mistral) | Lokales Modell, OpenAI-kompatibler API |
| **Enrichment (Online)** | n8n + Google Gemini | Bildanalyse via Gemini, orchestriert durch n8n-Workflow |
| **Fallback** | OpenAI GPT-4o | Backup wenn Ollama nicht verfÃ¼gbar |
| **PDF-Generation** | jsPDF | Client-seitig, keine Server-AbhÃ¤ngigkeit |

---

## ğŸ”„ Datenfluss

![Architektur](docs/datenfluss.svg)

---

## ğŸ§  Woher kommt das Fachwissen?

Die App nutzt **keine vorab trainierten Handwerks-Modelle**. Stattdessen wird eine strukturierte **Preisliste 2025 (Deutschland)** als System-Prompt an die KI Ã¼bergeben. Dies gewÃ¤hrleistet:

- âœ… **Volle Transparenz** â€” Jeder Preis ist nachvollziehbar
- âœ… **Anpassbarkeit** â€” Preise kÃ¶nnen jederzeit aktualisiert werden
- âœ… **Keine Black-Box** â€” Die gesamte Kalkulationslogik ist offen einsehbar

**Beispiel-Preise:**
- Wandanstrich weiÃŸ (2-fach): 12.50 â‚¬/mÂ²
- Spachteln Q2: 18.00 â‚¬/mÂ²
- Anfahrt (bis 30 km): 59.00 â‚¬ pauschal

Die Prompts enthalten auÃŸerdem:
- Faustformeln fÃ¼r FlÃ¤chenberechnungen
- Keyword-basierte Zustandserkennung ("Risse" â†’ Spachteln erforderlich)
- Kalkulationsreihenfolge (Vorarbeiten â†’ Untergrund â†’ Anstrich â†’ Zusatzkosten)

â†’ **[VollstÃ¤ndige Dokumentation der Wissensquelle](docs/WISSENSQUELLE.md)**

---

## ğŸ“¦ Installation & Setup

### Voraussetzungen

- Node.js 18+
- Rust (via rustup)
- [Ollama](https://ollama.ai) installiert
- macOS / Windows / Linux

### 1. Repository klonen

```bash
git clone https://github.com/somuchlearn/Everlast-contest-maler-app.git
cd Everlast-contest-maler-app
```

### 2. Ollama-Modell pullen *(nur fÃ¼r Offline-Modus)*

```bash
ollama pull mistral
```

> Das Modell (~4GB) wird einmal heruntergeladen und lokal gespeichert. Ollama muss beim Betrieb im Hintergrund laufen. Im Online-Modus wird Ollama nicht benÃ¶tigt.

### 3. Dependencies installieren

```bash
npm install
```

### 4. Environment konfigurieren (optional)

Erstelle `.env` im Root-Verzeichnis fÃ¼r den Cloud-Fallback:

```env
NEXT_PUBLIC_OPENAI_API_KEY=sk-your-openai-api-key
```

> Ohne diesen Key lÃ¤uft die App trotzdem â€“ solange Ollama lokal lÃ¤uft, wird OpenAI nicht benÃ¶tigt.

### 5. Development starten

```bash
npm run tauri dev
```

### 6. Production Build

```bash
npm run tauri build
```

Das Build-Artefakt liegt in `src-tauri/target/release/bundle/`.

> **Im Offline-Modus** wird beim ersten Umschalten das Whisper-Modell (~244MB) automatisch heruntergeladen und gecacht. Danach lÃ¤uft die Transkription komplett ohne Internet.

---

## âŒ¨ï¸ Bedienung

| Aktion | Shortcut / UI |
|--------|---------------|
| Online/Offline wechseln | Toggle-Schalter im Header (ğŸ–¥ï¸ â†” â˜ï¸) |
| App aktivieren | `âŒ˜â‡§R` (Mac) / `Ctrl+Shift+R` (Win/Linux) |
| Aufnahme starten | Button "ğŸ™ï¸ Aufnahme starten" oder Hotkey |
| Aufnahme stoppen | Button "Aufnahme stoppen" oder Hotkey erneut |
| Fotos hinzufÃ¼gen | Drag & Drop oder Klick auf Upload-Zone |
| Angebot erstellen | Button "Angebot erstellen â†’" |
| PDF exportieren | Button "ğŸ“„ PDF" im Ergebnis |
| Neues Angebot | Button "â†º Neu" |

---

## ğŸ¨ Design-Entscheidungen

### 1. Warum Tauri statt Electron?

| Kriterium | Tauri | Electron |
|-----------|-------|----------|
| Bundle-GrÃ¶ÃŸe | ~10 MB | ~150 MB |
| RAM-Verbrauch | ~50 MB | ~300 MB |
| Startup-Zeit | <1s | 2-3s |
| Sicherheit | Rust-basiert | Chromium-basiert |

**Fazit:** FÃ¼r eine App die "nahtlos im Workflow" sein soll, ist Performance entscheidend.

### 2. Warum Online/Offline-Toggle statt rein Cloud oder rein lokal?

Das Briefing fordert eine **"eigenstÃ¤ndige Desktop-Applikation"** â€” das erfordert Funktionieren ohne Internet. Gleichzeitig bietet Cloud-KI konkrete Vorteile (Bildanalyse, Geschwindigkeit). Der Toggle gibt dem Nutzer die Kontrolle:

- **Offline-Modus:** Whisper lÃ¤uft Ã¼ber `@huggingface/transformers` direkt im Browser (WASM). Enrichment per Ollama (Mistral) lokal. Falls Ollama nicht verfÃ¼gbar â†’ automatischer Fallback auf OpenAI GPT-4o.
- **Online-Modus:** Transkription Ã¼ber OpenAI Whisper API. Enrichment Ã¼ber n8n-Workflow mit Google Gemini â€” hier kommt die Bildanalyse ins Spiel (sieh Punkt 3).
- **Pragmatische Entscheidung:** Wer Ollama nicht installieren kann oder will, kann trotzdem die vollen Features nutzen â€“ einfach auf Online bleiben.

### 3. Warum n8n + Google Gemini fÃ¼r Online-Enrichment?

Die Foto-Analyse war die technisch anspruchsvolste Anforderung. Optionen:

- **OpenAI Vision direkt:** Einfacher, aber kein Workflow-Management, kein Routing nach Medientyp.
- **n8n + Gemini (gewÃ¤hlt):** n8n orchestriert den gesamten Prozess â€” Routing nach `text_only` / `image_analysis` / `mixed_media`, unterschiedliche Gemini-Modelle je KomplexitÃ¤t (2.5 Pro fÃ¼r Text, 1.5 Pro fÃ¼r Multi-Modal). Die Bilder werden als `multipart/form-data` an den Webhook gesendet und landen direkt in n8n's Binary-Pipeline.

**Vorteil:** Der KI-Workflow kann unabhÃ¤ngig vom Frontend weiterentwickelt werden â€” neue Modelle, Prompts oder Routingregeln Ã¤ndern sich nur auf der n8n-Seite.

### 4. Warum Ollama + Mistral?

- **OpenAI-kompatibler API:** Der gleiche Code arbeitet mit Ollama lokal und OpenAI als Fallback â€“ minimale Duplikation.
- **Mistral:** Starkes Modell fÃ¼r Deutsch, verlÃ¤sslich bei strukturierter JSON-Ausgabe.
- **Einfaches Setup:** Ein Befehl (`ollama pull mistral`), dann lÃ¤uft es.

### 5. UI/UX Entscheidungen

- **Dark Mode:** Reduziert Augenbelastung bei lÃ¤ngerer Nutzung
- **Minimalistisches Design:** Fokus auf Funktion, keine Ablenkung
- **Inline-Feedback:** Status, Transkript, Fehler direkt sichtbar
- **Model-Status-Indikator:** Zeigt ob das Whisper-Modell geladen ist

---

## ğŸ“Š Beispiel-Output

**Eingabe (Sprache):**
> "Schlafzimmer 18 Quadratmeter, WÃ¤nde streichen weiÃŸ, Decke auch, ein Fenster, TÃ¼r, kleiner Riss an der Nordwand"

**Ausgabe:**

| Position | Menge | Einheit | Preis | Summe |
|----------|-------|---------|-------|-------|
| Abkleben Fenster, TÃ¼r | 2 | Stk | 4,50 â‚¬ | 9,00 â‚¬ |
| Risse ausbessern | 1 | Stk | 30,00 â‚¬ | 30,00 â‚¬ |
| Grundierung WÃ¤nde | 48 | mÂ² | 8,00 â‚¬ | 384,00 â‚¬ |
| Wandanstrich weiÃŸ 2-fach | 48 | mÂ² | 14,00 â‚¬ | 672,00 â‚¬ |
| Deckenanstrich weiÃŸ | 18 | mÂ² | 13,00 â‚¬ | 234,00 â‚¬ |
| **Netto** | | | | **1.329,00 â‚¬** |
| **MwSt. 19%** | | | | **252,51 â‚¬** |
| **Brutto** | | | | **1.581,51 â‚¬** |

---

## âš ï¸ Known Issues & Security Considerations

### API-Key Exponierung (Client-Side)

**Problem:**
Die App nutzt `NEXT_PUBLIC_OPENAI_API_KEY` fÃ¼r den OpenAI-Fallback. Environment-Variablen mit dem `NEXT_PUBLIC_` Prefix werden in Next.js im Client-Bundle exponiert und kÃ¶nnen theoretisch aus dem JavaScript extrahiert werden.

**Risiko-Bewertung:**
- ğŸŸ¡ **Mittel** â€” Betrifft nur den Fallback-Pfad (Ollama ist primÃ¤r)
- User kÃ¶nnen die App vollstÃ¤ndig ohne OpenAI-Key nutzen (Offline-Modus)
- Im Offline-Modus werden keine Cloud-APIs benÃ¶tigt

**Best Practice LÃ¶sung:**
FÃ¼r Production-Deployments sollten sensible API-Keys nicht im Client-Bundle landen. Empfohlene AnsÃ¤tze:

1. **Tauri Command** (Desktop-Apps):
   ```rust
   #[tauri::command]
   fn get_api_key() -> Result<String, String> {
       std::env::var("OPENAI_API_KEY")
           .map_err(|_| "Key not configured".to_string())
   }
   ```
   Der Key liegt dann nur im Rust-Binary, nie im JavaScript.

2. **Backend-Proxy** (Web-Apps):
   ```
   Client â†’ Next.js API Route â†’ OpenAI
   ```
   API-Keys bleiben auf dem Server.

3. **Nur lokale Modelle** (Privacy-First):
   ```
   Ollama + Mistral (lokal) â†’ Kein API-Key nÃ¶tig
   ```

**FÃ¼r dieses Projekt:**
Der OpenAI-Fallback ist bewusst optional gehalten. Der Offline-Modus (Hauptfeature) funktioniert komplett ohne Cloud-APIs und demonstriert die Desktop-App-FÃ¤higkeiten fÃ¼r den Contest.

---

### n8n Webhook (Online-Modus)

**Implementierung:**
Der n8n-Webhook-Endpoint fÃ¼r die Online-Bildanalyse ist aus SicherheitsgrÃ¼nden nicht im Repository hart-codiert. Stattdessen wird er Ã¼ber die Environment-Variable `NEXT_PUBLIC_N8N_WEBHOOK_URL` konfiguriert.

**Lokale Entwicklung:**
Siehe `.env.example` fÃ¼r Setup-Anleitung.

**FÃ¼r Contest-Demo:**
Der Offline-Modus wird demonstriert (keine Cloud-AbhÃ¤ngigkeit, alle Features lokal verfÃ¼gbar).

**Best Practice:**
Webhooks sollten entweder durch zufÃ¤llige URLs, API-Key-Authentifizierung oder Request-Signing geschÃ¼tzt werden. Siehe `docs/SICHERHEIT.md` fÃ¼r Details.

---

## ğŸ™ Danksagungen

- [Tauri](https://tauri.app/) â€“ Desktop-Framework
- [Next.js](https://nextjs.org/) â€“ Web-Framework
- [@huggingface/transformers](https://github.com/huggingface/transformers.js) â€“ Lokale Whisper-Inferenz im Browser
- [Ollama](https://ollama.ai/) â€“ Lokale LLM-AusfÃ¼hrung
- [n8n](https://n8n.io/) â€“ Workflow-Orchestrierung fÃ¼r Online-Enrichment
- [Google Gemini](https://ai.google/) â€“ Cloud-KI fÃ¼r Text- und Bildanalyse
- [jsPDF](https://github.com/parallax/jsPDF) â€“ Client-seitige PDF-Generierung

---

<div align="center">

**ğŸ¨ MalerVoice AI** â€“ *Sprich dein Angebot.*

</div>
